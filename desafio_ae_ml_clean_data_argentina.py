# -*- coding: utf-8 -*-
"""Desafio_AE_ML_Clean_data_argentina.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1M8ds_9uHQrMQgcwNbw2xxhRK48ziX3fG

# Instalando e iniciando Spark
"""

!apt-get update -qq
!apt-get install openjdk-8-jdk-headless -qq > /dev/null
!wget -q https://archive.apache.org/dist/spark/spark-3.1.2/spark-3.1.2-bin-hadoop2.7.tgz
!tar xf spark-3.1.2-bin-hadoop2.7.tgz
!pip install -q findspark

import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"
os.environ["SPARK_HOME"] = "/content/spark-3.1.2-bin-hadoop2.7"

import findspark
findspark.init()

from pyspark.sql import SparkSession

spark = SparkSession.builder\
     .master('local[*]')\
    .appName("Iniciando sessão Spark")\
    .config("spark.ui.port", '4050')\
    .getOrCreate()

"""# Montandro drive para carregamento dos arquivos"""

from google.colab import drive
drive.mount('/content/drive')

"""# Carregando informações para sanitização de dados"""

path = '/content/drive/MyDrive/desafio_ML/data.csv'
df = spark.read.csv(path,sep=',',inferSchema=True, header=True)

path = '/content/drive/MyDrive/desafio_ML/indicator.csv'
dfi = spark.read.csv(path,sep=',',inferSchema=True, header=True)

"""Montando lista de indicadores para filtro posterior"""

coluna_desejada = "indicator_code"
lista_valores = dfi.select(coluna_desejada).rdd.flatMap(lambda x: x).collect()
print(lista_valores)

"""Criando dois dataFrames separados para valores em percentual e quantidade"""

from pyspark.sql.functions import col
coluna_filtrar = "indicator_code"
df_filtrado = df.filter(col(coluna_filtrar).isin(lista_valores))
dff = df_filtrado.filter(df_filtrado.country_code == 'ARG')
dfv = dff.filter(df_filtrado.indicator_code != 'IT.NET.USER.ZS')

df = dfv.toPandas()
print(df)

"""Pivoteando o DF"""

import pandas as pd

# Listando colunas
id_cols = ["country_name", "country_code", "indicator_name", "indicator_code"]

# Pivoteando o df
df_pivotado = pd.melt(df, id_vars=id_cols, var_name="Ano", value_name="Valor")

# Transformando em spark DF
sparkDF=spark.createDataFrame(df_pivotado)

"""Convertendo e sanitizando as colunas do DF"""

#sparkDF.show()
from pyspark.sql import functions
from pyspark.sql.types import IntegerType
from pyspark.sql.functions import format_number
df = sparkDF.withColumn("Valor", functions.regexp_replace('Valor',r'[,]',"."))
df = df.withColumn("Valor", df["Valor"].cast(IntegerType()))
df = df.withColumn("Valor", format_number("Valor",0))

# Salvando arquivo final

df_coalesced = df.coalesce(1)

caminho_google_drive = "/content/drive/MyDrive/desafio_AE_ML/data_argentina/"
nome_do_arquivo = "data_argentina_qtd.csv"

df_coalesced.write.csv(caminho_google_drive + nome_do_arquivo, header=True, mode="overwrite")